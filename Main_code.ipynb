{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narmathan56/AI-Math-Tutor/blob/main/Main_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7VgqHFDm_Z_",
        "outputId": "27f6c9bb-aaf1-4ed8-a386-524c2af00dec"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.12.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup\n"
      ],
      "metadata": {
        "id": "rOR29eFi8rpw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Setup\n",
        "!pip uninstall -y torchtext torch torchvision torchaudio\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0I0Ev4P6f7A",
        "outputId": "7a19cdcc-6972-4525-d190-5702d735a896"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.18.0\n",
            "Uninstalling torchtext-0.18.0:\n",
            "  Successfully uninstalled torchtext-0.18.0\n",
            "Found existing installation: torch 2.9.1\n",
            "Uninstalling torch-2.9.1:\n",
            "  Successfully uninstalled torch-2.9.1\n",
            "\u001b[33mWARNING: Skipping torchvision as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping torchaudio as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Version\n",
        "\n"
      ],
      "metadata": {
        "id": "GPVAa7urfaE4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gQH0tUJqkjz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sH4iMXI5kjwd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchtext==0.18.0 torch==2.3.0 -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTV9HVxUTeV3",
        "outputId": "cd48fa35-7bb2-4a86-a522-f84483389de0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.18.0\n",
            "  Using cached torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Collecting torch==2.3.0\n",
            "  Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.32.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchtext==0.18.0) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.3.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0) (12.8.93)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.3.0) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torchtext==0.18.0) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.3.0) (1.3.0)\n",
            "Using cached torchtext-0.18.0-cp312-cp312-manylinux1_x86_64.whl (2.0 MB)\n",
            "Downloading torch-2.3.0-cp312-cp312-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m118.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchtext\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
            "    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
            "    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "timm 1.0.22 requires torchvision, which is not installed.\n",
            "fastai 2.8.5 requires torchvision>=0.11, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchtext\n",
        "print(torch.__version__)\n",
        "print(torchtext.__version__)\n"
      ],
      "metadata": {
        "id": "rLGjyt9QxiBY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5579e76a-7287-45df-9195-f6c5ea5d2956"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n",
            "0.18.0+cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmKAAMJUxPk7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2b9b9d3-b11f-493a-9f53-b6d4ed8b0b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Available: NVIDIA L4\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"GPU not available. Running on CPU.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "print(f\"Current device: {torch.cuda.current_device()}\")\n",
        "print(f\"Device name: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0oBYlI4khYa",
        "outputId": "25bbc32b-ea90-4040-95b0-9e8a5795bfc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "CUDA device count: 1\n",
            "Current device: 0\n",
            "Device name: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8XP_or4w5yC",
        "outputId": "46ec2ecd-5fd7-4d16-f8c1-54f02623981d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 2.3.0+cu121\n",
            "Torchtext version: 0.18.0+cpu\n",
            "numpy 2.0.2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torchtext\n",
        "import numpy as np\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "print(\"Torchtext version:\", torchtext.__version__)\n",
        "print(\"numpy\",np.__version__)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "E-Rvy78KvRQO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "FTCh63WyyVgG",
        "outputId": "0bc9c644-a980-4871-e09f-75b420b38c36"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1408506528.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m     98\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    132\u001b[0m   )\n\u001b[1;32m    133\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVpWa7gJynXU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Create .kaggle directory if it doesn't exist\n",
        "os.makedirs(\"/root/.kaggle\", exist_ok=True)\n",
        "\n",
        "# Copy kaggle.json from Google Drive to .kaggle\n",
        "shutil.copy(\"/content/drive/MyDrive/kaggle.json\", \"/root/.kaggle/\")\n",
        "\n",
        "# Set correct permissions\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8LovieRzXDV"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d awsaf49/MATH-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Xk7_ybm01tt"
      },
      "outputs": [],
      "source": [
        "!unzip -n MATH-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAGvDJgIUlmB"
      },
      "outputs": [],
      "source": [
        "\n",
        "pip install fsspec==2023.9.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdTVwzPQVi7E"
      },
      "outputs": [],
      "source": [
        "!pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P634b06d-ZP3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# --- 1. Mount Google Drive ---\n",
        "# This connects your Colab environment to your Google Drive.\n",
        "# You will be prompted to authorize access.\n",
        "print(\"Mounting Google Drive...\")\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print(\"Google Drive mounted.\")\n",
        "\n",
        "# --- 2. Install the Hugging Face 'datasets' library ---\n",
        "# This is necessary to easily download the DeepMind dataset.\n",
        "print(\"\\nInstalling 'datasets' library...\")\n",
        "!pip install datasets\n",
        "print(\"'datasets' library installed.\")\n",
        "\n",
        "# --- 3. Define the DeepMind Dataset and Subsets to Download ---\n",
        "# The DeepMind Math Dataset is modular. You need to specify which parts you want.\n",
        "# Downloading ALL subsets can take a very long time and consume a lot of space.\n",
        "# Choose wisely based on your needs.\n",
        "# For demonstration, we'll download a few popular ones.\n",
        "\n",
        "deepmind_dataset_name = \"deepmind/math_dataset\"\n",
        "\n",
        "# List of DeepMind subsets to download.\n",
        "# You can find the full list on the Hugging Face dataset page:\n",
        "# https://huggingface.co/datasets/deepmind/math_dataset\n",
        "# Example: \"algebra__linear_1d\", \"arithmetic__add_or_sub\", etc.\n",
        "# If you want to download all, you'd need to iterate through all possible subset names.\n",
        "# For this example, let's pick a few to avoid excessively long download times.\n",
        "selected_subsets = [\n",
        "    \"algebra__linear_1d\",\n",
        "    \"arithmetic__mul\",\n",
        "    \"calculus__differentiate\",\n",
        "    \"numbers__base_conversion\"\n",
        "]\n",
        "\n",
        "# --- 4. Load (Download) the Dataset Subsets to Colab's Cache ---\n",
        "# When you call load_dataset, it first downloads the data to a local cache\n",
        "# (usually /root/.cache/huggingface/datasets/deepmind___math_dataset/).\n",
        "# We will then copy from this cache location to your Google Drive.\n",
        "print(f\"\\nLoading (downloading) selected DeepMind Math dataset subsets to Colab's cache...\")\n",
        "\n",
        "# Dictionary to hold the loaded datasets for each subset\n",
        "loaded_deepmind_data = {}\n",
        "\n",
        "for subset in selected_subsets:\n",
        "    print(f\"  - Loading subset: {subset}\")\n",
        "    # Load all splits for the subset to ensure all files are downloaded\n",
        "    try:\n",
        "        # Load all splits (train-easy, train-medium, train-hard, test)\n",
        "        dataset_dict = load_dataset(deepmind_dataset_name, subset)\n",
        "        loaded_deepmind_data[subset] = dataset_dict\n",
        "        print(f\"    Loaded {subset} with splits: {list(dataset_dict.keys())}\")\n",
        "    except Exception as e:\n",
        "        print(f\"    Error loading {subset}: {e}. Skipping this subset.\")\n",
        "\n",
        "print(\"\\nAll selected DeepMind Math dataset subsets downloaded to Colab's cache.\")\n",
        "\n",
        "# --- 5. Define Destination Path in Google Drive ---\n",
        "# Choose a logical folder in your Google Drive to save the dataset.\n",
        "# This folder will be created if it doesn't exist.\n",
        "google_drive_destination_base = '/content/drive/MyDrive/DeepMind_Math_Dataset_Downloaded/'\n",
        "os.makedirs(google_drive_destination_base, exist_ok=True)\n",
        "print(f\"\\nDestination folder in Google Drive: {google_drive_destination_base}\")\n",
        "\n",
        "# --- 6. Copy Downloaded Files from Cache to Google Drive ---\n",
        "# The Hugging Face datasets cache structure is typically:\n",
        "# /root/.cache/huggingface/datasets/{dataset_name}/{subset_name}/{version}/{hash}/\n",
        "# We need to find the specific data files within this structure.\n",
        "\n",
        "# Get the default cache directory for Hugging Face datasets\n",
        "hf_cache_dir = os.path.expanduser(\"~/.cache/huggingface/datasets\")\n",
        "deepmind_cache_path_prefix = os.path.join(hf_cache_dir, \"deepmind___math_dataset\")\n",
        "\n",
        "print(\"\\nCopying files to Google Drive...\")\n",
        "\n",
        "for subset in selected_subsets:\n",
        "    # Construct the expected cache path for the subset\n",
        "    # This part can be tricky as the exact hash part changes.\n",
        "    # A more robust way is to inspect loaded_deepmind_data[subset].cache_files\n",
        "    # However, for this dataset, it often creates a clear folder structure.\n",
        "\n",
        "    # Let's try to find the actual data directory within the cache\n",
        "    # This might require some inspection of the cache structure after loading\n",
        "    # A common pattern is: /root/.cache/huggingface/datasets/deepmind___math_dataset/{subset_name}/{version_hash}/\n",
        "\n",
        "    # A more reliable way to get the actual path:\n",
        "    if subset in loaded_deepmind_data:\n",
        "        # Get the first split (e.g., 'train') and then its cache_files\n",
        "        # The cache_files list contains dictionaries with 'path'\n",
        "        first_split_path = loaded_deepmind_data[subset][list(loaded_deepmind_data[subset].keys())[0]].cache_files[0]['path']\n",
        "\n",
        "        # The actual data directory is usually the parent of the parent of the file\n",
        "        # e.g., /root/.cache/huggingface/datasets/deepmind___math_dataset/algebra__linear_1d/1.0.0/hash/data/train-00000-of-00001.jsonl.gz\n",
        "        # We want to copy the 'data' folder or the parent of 'data'\n",
        "\n",
        "        # Let's find the base directory of the downloaded data for this subset\n",
        "        # This might vary, so we'll try to go up until we find the subset name in the path\n",
        "        current_path = os.path.dirname(first_split_path)\n",
        "        while os.path.basename(current_path) != subset and current_path != '/':\n",
        "            current_path = os.path.dirname(current_path)\n",
        "\n",
        "        # The actual data is usually inside a 'data' folder or directly under the hash folder\n",
        "        # For DeepMind Math, it's often directly under the hash folder, containing .jsonl.gz files\n",
        "        # Let's target the folder that contains the actual data files (e.g., '1.0.0/hash/')\n",
        "        # A simpler approach is to copy the entire subset folder from the cache\n",
        "\n",
        "        # Find the specific cache directory for the subset\n",
        "        # This assumes a structure like /root/.cache/huggingface/datasets/deepmind___math_dataset/algebra__linear_1d/1.0.0/some_hash/\n",
        "        # We want to copy 'some_hash' folder or its contents\n",
        "\n",
        "        # A safer approach: copy the entire dataset cache folder for the subset.\n",
        "        # This will copy the '1.0.0/hash' folders as well.\n",
        "\n",
        "        # Find the root cache directory for the specific subset\n",
        "        # Example: /root/.cache/huggingface/datasets/deepmind___math_dataset/algebra__linear_1d/\n",
        "        subset_cache_root = os.path.join(deepmind_cache_path_prefix, subset)\n",
        "\n",
        "        if os.path.exists(subset_cache_root):\n",
        "            destination_path = os.path.join(google_drive_destination_base, subset)\n",
        "            print(f\"  - Copying '{subset}' from cache to '{destination_path}'...\")\n",
        "            try:\n",
        "                # Use copytree to copy the entire directory\n",
        "                shutil.copytree(subset_cache_root, destination_path, dirs_exist_ok=True)\n",
        "                print(f\"    Successfully copied {subset}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"    Error copying {subset}: {e}\")\n",
        "        else:\n",
        "            print(f\"    Cache directory for {subset} not found at {subset_cache_root}. Skipping copy.\")\n",
        "    else:\n",
        "        print(f\"    Dataset for {subset} was not loaded. Skipping copy.\")\n",
        "\n",
        "print(\"\\nDeepMind Math Dataset files copied to Google Drive.\")\n",
        "print(f\"You can now find your dataset in: {google_drive_destination_base}\")\n",
        "print(\"You can verify by navigating to this folder in your Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgYS_A1AYz_L"
      },
      "outputs": [],
      "source": [
        "subset = 'algebra__linear_1d'\n",
        "split_name = 'train'\n",
        "\n",
        "print(loaded_deepmind_data[subset][split_name].cache_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEq1A1ovZgcS"
      },
      "outputs": [],
      "source": [
        "save_dir = '/content/drive/MyDrive/DeepMind_Math_Dataset_Downloaded/'\n",
        "os.makedirs(save_dir,exist_ok=True)\n",
        "for subset in loaded_deepmind_data:\n",
        "  for split in loaded_deepmind_data[subset]:\n",
        "    dataset=loaded_deepmind_data[subset][split]\n",
        "    save_path=os.path.join(save_dir,f\"{subset}_{split}.json\")\n",
        "    print(f\"Saving {subset} [{split}] to {save_path}...\")\n",
        "    dataset.to_json(save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7H9IcdaeAak"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.listdir(\"drive/MyDrive/DeepMind_Math_Dataset_Downloaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjjJq7YTfP_k"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "algebra_train_data=pd.read_json(\"/content/drive/MyDrive/DeepMind_Math_Dataset_Downloaded/algebra__linear_1d_train.json\", lines=True)\n",
        "algebra_test_data=pd.read_json(\"/content/drive/MyDrive/DeepMind_Math_Dataset_Downloaded/algebra__linear_1d_test.json\", lines=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uyM6n-626Xl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.listdir()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdmy8tQ7h-B-"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(os.getcwd())  # Shows the current working directory\n",
        "print(os.listdir()) # Lists available directories and files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYckqIDt37Ur"
      },
      "outputs": [],
      "source": [
        "os.listdir('MATH')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBhnu-Eq4HEd"
      },
      "outputs": [],
      "source": [
        "os.listdir('MATH/train')\n",
        "os.listdir('MATH/test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hqj0xv4FM0K"
      },
      "source": [
        "##Load the File\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z6zsNXLmnoRL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Function to read JSON files into a pandas DataFrame\n",
        "def load_json_files(folder_path):\n",
        "    json_files = os.listdir(folder_path)  # Get list of all JSON files in the folder\n",
        "\n",
        "    data = []\n",
        "    for file in json_files:\n",
        "        if file.endswith('.json'):  # Check if the file is a JSON file\n",
        "            with open(os.path.join(folder_path, file), 'r') as f:\n",
        "                data.append(json.load(f))  # Load the JSON data\n",
        "    return pd.DataFrame(data)  # Convert the list of JSON data into a DataFrame\n",
        "\n",
        "# Load the data for training and testing\n",
        "train_data = load_json_files(\"MATH/train/algebra\")\n",
        "test_data = load_json_files(\"MATH/test/algebra\")\n",
        "\n",
        "# Display the first few rows\n",
        "print(train_data.head())\n",
        "print(test_data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Dataset Creation\n",
        "This is the part  of the dataset creation\n",
        "I am using cvs file and js file to keep the Datasets.\n",
        "I am using Pandas framework for access the datasets\n"
      ],
      "metadata": {
        "id": "zRvtUegDfrWe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b4b813-62cd-4fcc-d147-1f7ef916d202",
        "id": "4apg7uRQm9Xp"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 3)\n",
            "            question                                              steps answer\n",
            "0       What is 8+8?  Step 1: Add the ones → 8 + 8 = 16. So, the ans...     16\n",
            "1           7 plus 0  Step 1: Add the ones → 7 + 0 = 7. So, the answ...      7\n",
            "2     Calculate 3+8.  Step 1: Add the ones → 3 + 8 = 11. So, the ans...     11\n",
            "3           5 plus 1  Step 1: Add the ones → 5 + 1 = 6. So, the answ...      6\n",
            "4                0+5  Step 1: Add the ones → 0 + 5 = 5. So, the answ...      5\n",
            "...              ...                                                ...    ...\n",
            "1005     What is 1+1           Add the ones → 1 + 1 = 2 so, Answer is 2      2\n",
            "1006     What is 1+1           Add the ones → 1 + 1 = 2 so, Answer is 2      2\n",
            "1007     What is 1+1           Add the ones → 1 + 1 = 2 so, Answer is 2      2\n",
            "1008     What is 1+1           Add the ones → 1 + 1 = 2 so, Answer is 2      2\n",
            "1009     What is 1+1           Add the ones → 1 + 1 = 2 so, Answer is 2      2\n",
            "\n",
            "[1010 rows x 3 columns]\n",
            "(647, 3)\n",
            "Step 1: Add the ones → 7 + 0 = 7. So, the answer is 7. Answer: 7\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('/content/drive/MyDrive/one_digit_addition_varied_1000.csv')\n",
        "print(data.shape)\n",
        "# Assuming 'data' is your existing DataFrame\n",
        "new_rows = [\n",
        "    {'question': 'What is 1+1', 'steps': 'Add the ones → 1 + 1 = 2 so, Answer is 2', 'answer': '2'},\n",
        "     {'question': 'What is 1 plus 1', 'steps': 'Add the ones → 1 + 1 = 2 so, Answer is 2', 'answer': '2'},\n",
        "     {'question': 'What is 2+2', 'steps': 'Add the ones → 2 + 2 = 4 so, Answer is 4', 'answer': '4'},\n",
        "     {'question': 'What is 2 plus 2', 'steps': 'Add the ones → 2 + 2 = 4 so, Answer is 4', 'answer': '4'},\n",
        "     {'question': 'What is 3+3', 'steps': 'Add the ones → 3 + 3 = 6 so, Answer is 6', 'answer': '6'},\n",
        "     {'question': 'What is 1+1', 'steps': 'Add the ones → 1 + 1 = 2 so, Answer is 2', 'answer': '2'},\n",
        "     {'question': 'What is 1+1', 'steps': 'Add the ones → 1 + 1 = 2 so, Answer is 2', 'answer': '2'},\n",
        "     {'question': 'What is 1+1', 'steps': 'Add the ones → 1 + 1 = 2 so, Answer is 2', 'answer': '2'},\n",
        "     {'question': 'What is 1+1', 'steps': 'Add the ones → 1 + 1 = 2 so, Answer is 2', 'answer': '2'},\n",
        "     {'question': 'What is 1+1', 'steps': 'Add the ones → 1 + 1 = 2 so, Answer is 2', 'answer': '2'},\n",
        "\n",
        "]\n",
        "\n",
        "# Convert the list of dictionaries to a DataFrame\n",
        "new_rows_df = pd.DataFrame(new_rows)\n",
        "\n",
        "# Append them to the existing DataFrame\n",
        "data = pd.concat([data, new_rows_df], ignore_index=True)\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(data)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data=data.drop_duplicates()\n",
        "print(data.shape)\n",
        "\n",
        "\n",
        "X_train=data['question']\n",
        "y_train = data['steps'] + \" Answer: \" + data['answer'].astype(str)\n",
        "\n",
        "\n",
        "X_test=data['question']\n",
        "y_test= data['steps'] + \" Answer: \" + data['answer'].astype(str)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(y_train[30])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO2MryDVhOOe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "data=pd.read_csv('/content/drive/MyDrive/one_digit_addition_varied_1000.csv')\n",
        "print(data.shape)\n",
        "\n",
        "data.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data=data.drop_duplicates()\n",
        "print(data.shape)\n",
        "\n",
        "\n",
        "X_train=data['question']\n",
        "y_train = data['steps'] + \" Answer: \" + data['answer'].astype(str)\n",
        "\n",
        "\n",
        "print(y_train[30])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/drive/MyDrive/one_digit_addition_250.csv')\n",
        "data.head()\n",
        "data=data.drop_duplicates()\n",
        "print(data.info)\n",
        "X_test=data['question']\n",
        "y_test= data['steps'] + \" Answer: \" + data['answer'].astype(str)# Ensure y_test is string type\n",
        "\n",
        "print(X_test.head())\n",
        "print(X_test.shape)\n",
        "print(X_train.info)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "AAlp0AXzq33Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGS_XR_UBQ9b"
      },
      "source": [
        "##Splitting the Data\n",
        "splitting the data as X_train,Y_train,X_test,Y_test.\n",
        "X_train is independant variable according to math tutor this is the problems.\n",
        "Y_train is dependant variable according to math tutor this is the solution we can say as Target lables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G65TTvCltbqE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "print(algebra_train_data.shape)\n",
        "print(algebra_test_data.shape)\n",
        "algebra_train_data=algebra_train_data.rename(columns={'question': 'problem', 'answer': 'solution'})\n",
        "algebra_test_data=algebra_test_data.rename(columns={'question': 'problem', 'answer': 'solution'})\n",
        "algebra_X_train=algebra_train_data['problem']\n",
        "algebra_y_train=algebra_train_data['solution']\n",
        "algebra_X_test=algebra_test_data['problem']\n",
        "algebra_y_test=algebra_test_data['solution']\n",
        "print(algebra_X_train.shape)\n",
        "print(algebra_y_train.shape)\n",
        "print(algebra_X_test.shape)\n",
        "print(algebra_y_test.shape)\n",
        "print(algebra_X_train.head)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GpD6L1nj3HBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8qOvqsjnPtQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"train_data:\",train_data.shape)\n",
        "print(\"test_data:\",test_data.shape)\n",
        "# Example: Splitting the data into 80% training and 20% testing\n",
        "X_train = train_data['problem']  # Features (e.g., text of math problems)\n",
        "y_train = train_data['solution']\n",
        "X_test=test_data['problem']\n",
        "y_test=test_data['solution']\n",
        "print(\"Y_train:\",X_train.shape)\n",
        "print(\"Y_train:\",y_train.shape)\n",
        "print(\"X_test:\",X_test.shape)\n",
        "print(\"y_test:\",y_test.shape)\n",
        "print(X_train)\n",
        "print(y_train)\n",
        "print(X_test)\n",
        "print(y_test)\n",
        "print(\"Example Math Problem:\", X_train.iloc[2])\n",
        "print(\"Corresponding Solution:\", y_train.iloc[1])\n",
        "print(\"Example Math Problem:\", X_test.iloc[0])\n",
        "print(\"Corresponding Solution:\", y_test.iloc[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-fp8bYWzV_W"
      },
      "source": [
        "##merging all **data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9XbOlM4zUpb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "X_train = pd.concat([algebra_X_train, algebra_X_test])\n",
        "y_train = pd.concat([algebra_y_train, algebra_y_test])\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_train.head())\n",
        "print(y_train.head())\n",
        "print(X_train.tail())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fch7j_QM-RsD"
      },
      "outputs": [],
      "source": [
        "# Define the desired number of samples\n",
        "train_samples = 10000\n",
        "test_samples = 1000\n",
        "\n",
        "# Sample the datasets\n",
        "# Use .sample() to randomly select rows\n",
        "# use .reset_index(drop=True) to reset the index after sampling\n",
        "X_train_sampled = X_train.sample(n=min(train_samples, len(X_train)), random_state=42).reset_index(drop=True)\n",
        "y_train_sampled = y_train.sample(n=min(train_samples, len(y_train)), random_state=42).reset_index(drop=True) # Use the same random_state for consistency if needed, or sample independently\n",
        "\n",
        "X_test_sampled = X_test.sample(n=min(test_samples, len(X_test)), random_state=42).reset_index(drop=True)\n",
        "y_test_sampled = y_test.sample(n=min(test_samples, len(y_test)), random_state=42).reset_index(drop=True) # Use the same random_state for consistency if needed, or sample independently\n",
        "\n",
        "\n",
        "print(\"Sampled X_train shape:\", X_train_sampled.shape)\n",
        "print(\"Sampled y_train shape:\", y_train_sampled.shape)\n",
        "print(\"Sampled X_test shape:\", X_test_sampled.shape)\n",
        "print(\"Sampled y_test shape:\", y_test_sampled.shape)\n",
        "\n",
        "# Now, use these sampled variables in your data cleaning cell\n",
        "X_train = X_train_sampled\n",
        "y_train = y_train_sampled\n",
        "X_test = X_test_sampled\n",
        "y_test = y_test_sampled"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go8qSYy1Fdwi"
      },
      "source": [
        "## Data Cleaning\n",
        "We are using the Torch text for data cleaning and numeral encoding\n",
        "first step data cleaning means we are consentrate on missing values,repeated values\n",
        "2.tokenizing this means we have to split the text as sup part to change as numral encodings\n",
        "3.numaral encodings this means if we gonna feed the data into model, first, we need to change as tensors which include some numbers that numbers created by build  vocubulary concept.\n",
        "once tokenized the text after build vocubulary there are some numbers assigned with perticular text\n",
        "[the:1,cat:2].\n",
        "after we change to tensor\n",
        "\n",
        "\n",
        "Clean → Tokenize → Encode → Tensorize ✅"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(type(X_train.iloc[0]))\n"
      ],
      "metadata": {
        "id": "puFujBhckijC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86f9b7d-421b-413f-ea81-075862cefd32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3euiAaNosXRx",
        "outputId": "5303cd60-1efb-43d3-f140-dcfb119433f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.12/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sample Cleaned Output: 0                what is 8 + 8?\n",
            "1                      7 plus 0\n",
            "2              calculate 3 + 8.\n",
            "3                      5 plus 1\n",
            "4                         0 + 5\n",
            "                 ...           \n",
            "100        can you add 8 and 0?\n",
            "101                add 5 and 3.\n",
            "102                    9 plus 9\n",
            "103                       9 + 1\n",
            "104    find the sum of 4 and 5.\n",
            "Name: question, Length: 100, dtype: object\n",
            "✅ Sample Cleaned Output: step 1: add the ones → 8 + 8 = 16. so, the answer is 16. answer: 16\n",
            "X_train_cleaned_type <class 'str'>\n",
            "✅ Preprocessing Done!\n",
            "X_train_tensor shape: torch.Size([647, 10])\n",
            "y_train_tensor shape: torch.Size([647, 30])\n",
            "X_test_tensor shape: torch.Size([647, 10])\n",
            "y_test_tensor shape: torch.Size([647, 30])\n",
            "📚 Vocabulary size: 51\n",
            "tensor([31, 10, 17,  8, 17, 25,  1,  1,  1,  1])\n",
            "tensor([ 2, 16,  7,  9,  5, 13, 15, 17,  8, 17, 12, 48,  4, 14, 11,  5,  6, 10,\n",
            "        48,  4,  6, 48,  3,  1,  1,  1,  1,  1,  1,  1])\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.functional import pad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import vocab as torch_vocab\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Step 1: Clean text function\n",
        "# Decode entire column first\n",
        "X_train = X_train.apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
        "y_train = y_train.apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
        "X_test = X_test.apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
        "y_test = y_test.apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
        "\n",
        "def clean_text(text):\n",
        "    if text is None or pd.isna(text):\n",
        "       return \"\"\n",
        "   # if(text.startswith(\"b'\")and text.endswith(\"'\")):\n",
        "\n",
        "       # text=text[2:-3].strip()\n",
        "    # Decode bytes literal if necessary\n",
        "    if isinstance(text, bytes):\n",
        "        text = text.decode(\"utf-8\")\n",
        "    # Assuming UTF-8 encoding\n",
        "\n",
        "    # Extract labels from Asymptote block before removing it\n",
        "    asy_labels = re.findall(r'label\\([^,]+,\"([^\"]+)\"', text)\n",
        "\n",
        "    # Remove entire [asy] block\n",
        "    text = re.sub(r'\\[asy\\].*?\\[/asy\\]','', text, flags=re.DOTALL)\n",
        "\n",
        "\n",
        "\n",
        "    # Remove LaTeX math mode ($...$), LaTeX commands like \\mbox{}, etc.\n",
        "    text = re.sub(r'\\$\\\\?([^$]+)\\\\?\\$', r'\\1', text)\n",
        "    text = re.sub(r'\\\\\\w+\\{(.*?)\\}', r'\\1', text)\n",
        "    text = text.replace('\\\\', '')\n",
        "\n",
        "    # Normalize whitespaces and math operators\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\s*([+\\-*/=()^])\\s*', r' \\1 ', text)\n",
        "\n",
        "    # Fix parentheses spacing\n",
        "    text = re.sub(r'\\s*\\(\\s*', '(', text)\n",
        "    text = re.sub(r'\\s*\\)\\s*', ')', text)\n",
        "\n",
        "    # Lowercase everything\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove trailing newline characters explicitly if not removed by strip()\n",
        "    text = text.rstrip('\\n')\n",
        "\n",
        "\n",
        "    # Add logic sentence from Asymptote if present\n",
        "    if asy_labels:\n",
        "        logic_steps = [label.lower() for label in asy_labels if \"input\" not in label and \"output\" not in label]\n",
        "        logic_sentence = \" the machine \" + \", then \".join(logic_steps) + \".\" if logic_steps else \"\"\n",
        "\n",
        "        input_value = next((label.split('=')[1].strip() for label in asy_labels if \"input\" in label), None)\n",
        "\n",
        "        if input_value:\n",
        "            text = f\"in the function machine, the input is {input_value}. {text}{logic_sentence} what is the output?\"\n",
        "        else:\n",
        "            text = f\"{text}{logic_sentence} what is the output?\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# Step 2: Clean datasets\n",
        "# we are insert the system which've already made by us\n",
        "X_train_cleaned = X_train.apply(clean_text)\n",
        "y_train_cleaned = y_train.astype(str).apply(clean_text)\n",
        "X_test_cleaned = X_test.apply(clean_text)\n",
        "y_test_cleaned = y_test.astype(str).apply(clean_text)\n",
        "\n",
        "print(\"✅ Sample Cleaned Output:\", X_train_cleaned.iloc[0 :100])\n",
        "print(\"✅ Sample Cleaned Output:\", y_train_cleaned.iloc[0])\n",
        "print(\"X_train_cleaned_type\",type(X_train.iloc[0]))\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "X_train_tokens = X_train_cleaned.apply(tokenizer)\n",
        "y_train_tokens = y_train_cleaned.apply(tokenizer)\n",
        "X_test_tokens = X_test_cleaned.apply(tokenizer)\n",
        "y_test_tokens = y_test_cleaned.apply(tokenizer)\n",
        "\n",
        "# Step 4: Build vocab from all tokens\n",
        "def yield_tokens(*data_parts):\n",
        "    for part in data_parts:\n",
        "        for tokens in part:\n",
        "            yield tokens\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(X_train_tokens, y_train_tokens, X_test_tokens, y_test_tokens),\n",
        "    specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"],\n",
        "    max_tokens=5000\n",
        ")\n",
        "\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Step 5: Numericalize\n",
        "def numericalize(token_list, add_bos_eos=False):\n",
        "    if add_bos_eos:\n",
        "        token_list = ['<bos>'] + token_list + ['<eos>']\n",
        "    return torch.tensor([vocab[token] for token in token_list])\n",
        "\n",
        "X_train_ids = X_train_tokens.apply(lambda x: numericalize(x))\n",
        "y_train_ids = y_train_tokens.apply(lambda x: numericalize(x, add_bos_eos=True))\n",
        "X_test_ids = X_test_tokens.apply(lambda x: numericalize(x))\n",
        "y_test_ids = y_test_tokens.apply(lambda x: numericalize(x, add_bos_eos=True))\n",
        "\n",
        "# Step 6: Padding\n",
        "def pad_sequences(sequences, max_len):\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        # Ensure seq is a tensor before padding\n",
        "        if not isinstance(seq, torch.Tensor):\n",
        "            seq = torch.tensor(seq)\n",
        "\n",
        "        current_len = seq.size(0)\n",
        "        if current_len < max_len:\n",
        "            padded_seq = pad(seq, (0, max_len - current_len), value=vocab[\"<pad>\"])\n",
        "        else:\n",
        "            padded_seq = seq[:max_len]\n",
        "        padded_sequences.append(padded_seq)\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "X_train_tensor = pad_sequences(X_train_ids.tolist(), max_len=10)\n",
        "y_train_tensor = pad_sequences(y_train_ids.tolist(), max_len=30)\n",
        "X_test_tensor = pad_sequences(X_test_ids.tolist(), max_len=10)\n",
        "y_test_tensor = pad_sequences(y_test_ids.tolist(), max_len=30)\n",
        "\n",
        "\n",
        "# Step 7: Stack into final tensors\n",
        "X_train_tensor = torch.stack(X_train_tensor)\n",
        "y_train_tensor = torch.stack(y_train_tensor)\n",
        "X_test_tensor = torch.stack(X_test_tensor)\n",
        "y_test_tensor = torch.stack(y_test_tensor)\n",
        "\n",
        "\n",
        "# Step 8: Output shapes\n",
        "print(\"✅ Preprocessing Done!\")\n",
        "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
        "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
        "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
        "print(\"y_test_tensor shape:\", y_test_tensor.shape)\n",
        "print(\"📚 Vocabulary size:\", len(vocab))\n",
        "print(X_train_tensor[0])\n",
        "print(y_train_tensor[0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = X_train_tensor[:, 0].tolist()\n",
        "y = X_train_tensor[:, 1].tolist()\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uM8uwzBr-W5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids=X_train_tensor[0]\n",
        "decoded_tokens = [vocab.lookup_token(i)for i in ids if i !=vocab['<pad>']]\n",
        "decoded=' '.join(decoded_tokens)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "vGApQvXemESR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySBccZhj1x9x"
      },
      "outputs": [],
      "source": [
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NifvjYKP5d3"
      },
      "outputs": [],
      "source": [
        "print(\"Random example:\", y_train_tokens.iloc[0])\n",
        "print(\"Token IDs:\", [vocab[token] for token in y_train_tokens.iloc[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx_-djqmrbbh"
      },
      "source": [
        "##Create the DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N6UF8lmck_Wp"
      },
      "outputs": [],
      "source": [
        "print(X_batch.shape)\n",
        "print(y_batch.shape)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6aYRM6Vstdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# -----------------------------\n",
        "# 1️⃣ Create TensorDataset\n",
        "# -----------------------------\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# -----------------------------\n",
        "# 2️⃣ Create DataLoaders\n",
        "# -----------------------------\n",
        "batch_size = 16  # you can adjust\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# -----------------------------\n",
        "# 3️⃣ Check one batch\n",
        "# -----------------------------\n",
        "for x_batch, y_batch in train_loader:\n",
        "    print(\"Input batch shape:\", x_batch.shape)\n",
        "    print(\"Target batch shape:\", y_batch.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTm3MAq6p92H",
        "outputId": "953ad392-1f1c-4913-b8ea-ee273cd1b37b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input batch shape: torch.Size([16, 10])\n",
            "Target batch shape: torch.Size([16, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz1hKmPOHEkm"
      },
      "source": [
        "#Create Transfomer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BPExx2pgGrd-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941
        },
        "outputId": "7fce846f-a666-44c0-c5dd-9e177c4b579f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using device: cuda\n",
            "✅ Epoch 1/26, Loss: 2.7276\n",
            "✅ Epoch 2/26, Loss: 1.1731\n",
            "✅ Epoch 3/26, Loss: 0.7041\n",
            "✅ Epoch 4/26, Loss: 0.5335\n",
            "✅ Epoch 5/26, Loss: 0.4372\n",
            "✅ Epoch 6/26, Loss: 0.3739\n",
            "✅ Epoch 7/26, Loss: 0.3195\n",
            "✅ Epoch 8/26, Loss: 0.2725\n",
            "✅ Epoch 9/26, Loss: 0.2235\n",
            "✅ Epoch 10/26, Loss: 0.1794\n",
            "✅ Epoch 11/26, Loss: 0.1483\n",
            "✅ Epoch 12/26, Loss: 0.1245\n",
            "✅ Epoch 13/26, Loss: 0.1056\n",
            "✅ Epoch 14/26, Loss: 0.0924\n",
            "✅ Epoch 15/26, Loss: 0.0804\n",
            "✅ Epoch 16/26, Loss: 0.0723\n",
            "✅ Epoch 17/26, Loss: 0.0636\n",
            "✅ Epoch 18/26, Loss: 0.0576\n",
            "✅ Epoch 19/26, Loss: 0.0521\n",
            "✅ Epoch 20/26, Loss: 0.0501\n",
            "✅ Epoch 21/26, Loss: 0.0447\n",
            "✅ Epoch 22/26, Loss: 0.0408\n",
            "✅ Epoch 23/26, Loss: 0.0367\n",
            "✅ Epoch 24/26, Loss: 0.0358\n",
            "✅ Epoch 25/26, Loss: 0.0323\n",
            "✅ Epoch 26/26, Loss: 0.0295\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"✅ Using device:\", device)\n",
        "\n",
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "\n",
        "# Transformer Model\n",
        "class MathTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size) # Use the passed vocab_size\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask=None, tgt_mask=None):\n",
        "        src = self.embedding(src)\n",
        "        src = self.positional_encoding(src)\n",
        "\n",
        "        tgt = self.embedding(tgt)\n",
        "        tgt = self.positional_encoding(tgt)\n",
        "\n",
        "        output = self.transformer(\n",
        "            src, tgt,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "        return self.fc_out(output)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_square_subsequent_mask(sz):\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
        "        return mask.masked_fill(mask, float('-inf')).masked_fill(~mask, float(0.0))\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_padding_mask(seq, pad_token_id):\n",
        "        return seq == pad_token_id\n",
        "\n",
        "    @staticmethod\n",
        "    def handle_vocab_size(X_batch, y_batch, vocab_size, unk_token=1):\n",
        "        X_batch = X_batch.clone()\n",
        "        y_batch = y_batch.clone()\n",
        "        X_batch[X_batch >= vocab_size] = unk_token\n",
        "        y_batch[y_batch >= vocab_size] = unk_token\n",
        "        return X_batch, y_batch\n",
        "\n",
        "# Dataset\n",
        "class MathDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab) # Use the actual vocab size here\n",
        "embed_size = 1024\n",
        "num_heads = 32\n",
        "num_layers =6\n",
        "dropout = 0.1\n",
        "pad_token_id = vocab[\"<pad>\"]  # Get pad token id from vocab\n",
        "num_epochs = 26\n",
        "\n",
        "\n",
        "# Initialize model, loss, optimizer, scheduler\n",
        "model = MathTransformer(vocab_size, embed_size, num_heads, num_layers, dropout).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8)\n",
        "\n",
        "# Training loop\n",
        "loss_values = []\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # Handle vocab overflow\n",
        "        X_batch, y_batch = MathTransformer.handle_vocab_size(X_batch, y_batch, vocab_size)\n",
        "\n",
        "        # Padding and masks\n",
        "        src_padding_mask = MathTransformer.generate_padding_mask(X_batch, pad_token_id).to(device)\n",
        "        tgt_input = y_batch[:, :-1]\n",
        "        tgt_output = y_batch[:, 1:]\n",
        "\n",
        "        tgt_mask = MathTransformer.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = model(X_batch, tgt_input, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "        # Reshape for loss\n",
        "        output = output.reshape(-1, vocab_size)\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt_output)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    loss_values.append(avg_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"✅ Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Plot loss\n",
        "plt.plot(loss_values, marker='o', label=\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training Loss Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsVdxgzk54OI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.save(model.state_dict(), \"math_transformer_weights.pth\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "g_3dcxOv58fR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "712a096e-dba5-4fdd-f724-9f0790c6c407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating model from cell BPExx2pgGrd-...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:408: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Evaluation Loss: 0.0821, Token Accuracy (excluding padding): 0.9787, BLEU Score: 0.9428\n",
            "\n",
            "Question: add 4 and 2.\n",
            "Predicted Solution: step 1 add the ones → 2 + 4 = 6 . so , the answer is 6 . answer 6\n",
            "Actual Solution: step 1: add the ones → 4 + 2 = 6. so, the answer is 6. answer: 6\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchtext.data.metrics import bleu_score # Import bleu_score\n",
        "\n",
        "# Define the evaluate function\n",
        "def evaluate(model, dataloader, criterion, device, vocab, pad_token_id):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    total_tokens = 0\n",
        "    all_predicted_solutions = []\n",
        "    all_target_solutions = []\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
        "        for X_batch, y_batch in dataloader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Ensure batch tensors do not contain indices out of vocab range\n",
        "            # Assuming the model being evaluated is MathTransformer or has a similar method\n",
        "            # If not, you might need to adjust this line or handle vocab overflow differently\n",
        "            X_batch, y_batch = MathTransformer.handle_vocab_size(X_batch, y_batch, len(vocab), vocab[\"<unk>\"])\n",
        "\n",
        "            # Padding mask (if needed for your transformer model)\n",
        "            src_padding_mask = MathTransformer.generate_padding_mask(X_batch, pad_token_id).to(device)\n",
        "            tgt_input = y_batch[:, :-1]  # All tokens except the last one\n",
        "            tgt_output = y_batch[:, 1:]  # All tokens except the first one\n",
        "\n",
        "            # Generate the target mask for the transformer\n",
        "            tgt_mask = MathTransformer.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(X_batch, tgt_input, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "\n",
        "            # Reshape for loss calculation\n",
        "            output_reshaped = output.reshape(-1, len(vocab))\n",
        "            tgt_output_reshaped = tgt_output.reshape(-1)\n",
        "\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(output_reshaped, tgt_output_reshaped)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Calculate correct predictions (optional, depends on your task)\n",
        "            _, predicted_tokens = output_reshaped.max(1)\n",
        "            # We only count correct predictions for non-padding tokens in the reshaped tensors\n",
        "            mask = (tgt_output_reshaped != pad_token_id)\n",
        "            correct_predictions += (predicted_tokens[mask] == tgt_output_reshaped[mask]).sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "\n",
        "\n",
        "            # Decode predictions and targets for BLEU score\n",
        "            itos = vocab.get_itos()\n",
        "            special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "            # Reshape predicted_tokens and tgt_output back to batch size and sequence length\n",
        "            predicted_tokens_batch = predicted_tokens.view(y_batch.size(0), -1)\n",
        "            tgt_output_batch = tgt_output.view(y_batch.size(0), -1)\n",
        "\n",
        "\n",
        "            for i in range(y_batch.size(0)):\n",
        "                predicted_seq = [itos[idx.item()] for idx in predicted_tokens_batch[i] if itos[idx.item()] not in special_tokens]\n",
        "                target_seq = [itos[idx.item()] for idx in tgt_output_batch[i] if itos[idx.item()] not in special_tokens]\n",
        "\n",
        "                # BLEU score expects a list of reference sentences for each hypothesis\n",
        "                all_predicted_solutions.append(predicted_seq)\n",
        "                all_target_solutions.append([target_seq]) # Wrap target_seq in a list\n",
        "\n",
        "\n",
        "    # Compute the average loss and accuracy\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    # Avoid division by zero if total_tokens is 0\n",
        "    accuracy = correct_predictions / total_tokens if total_tokens > 0 else 0.0\n",
        "\n",
        "    # Compute BLEU score\n",
        "    # Ensure that all_predicted_solutions and all_target_solutions are not empty\n",
        "    bleu = bleu_score(all_predicted_solutions, all_target_solutions) if all_predicted_solutions and all_target_solutions else 0.0\n",
        "\n",
        "\n",
        "    print(f\"✅ Evaluation Loss: {avg_loss:.4f}, Token Accuracy (excluding padding): {accuracy:.4f}, BLEU Score: {bleu:.4f}\")\n",
        "    return avg_loss, accuracy, bleu\n",
        "\n",
        "# Define reverse_vocab\n",
        "reverse_vocab = {idx: token for token, idx in vocab.get_stoi().items()}  # for torchtext Vocab\n",
        "\n",
        "\n",
        "# Define the predict_solution function (copied from cell xrU9SEDH3s29)\n",
        "def predict_solution(model, question, vocab, reverse_vocab, tokenizer, device, max_len_input, max_len_output, pad_token_id):\n",
        "    \"\"\"\n",
        "    Predicts the solution to a math question using a trained Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Transformer model\n",
        "        question: String, math question to solve\n",
        "        vocab: Vocab object (token -> index)\n",
        "        reverse_vocab: dict (index -> token) for decoding\n",
        "        tokenizer: Function to tokenize the question\n",
        "        device: torch.device\n",
        "        max_len_input: Max length of input sequence\n",
        "        max_len_output: Max length of output sequence\n",
        "        pad_token_id: ID of the padding token\n",
        "\n",
        "    Returns:\n",
        "        predicted_solution: String, the model's predicted solution\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # 1️⃣ Preprocess the question\n",
        "        cleaned_question = clean_text(question)  # your text cleaning function\n",
        "        tokenized_question = tokenizer(cleaned_question)\n",
        "        numericalized_question = numericalize(tokenized_question)  # convert tokens -> indices\n",
        "\n",
        "        # 2️⃣ Pad input sequence\n",
        "        # padded_question = pad_sequences([numericalized_question], max_len_input)\n",
        "        # Correct way to handle padding for a single sequence:\n",
        "        current_len = numericalized_question.size(0)\n",
        "        if current_len < max_len_input:\n",
        "            padded_question_tensor = pad(numericalized_question, (0, max_len_input - current_len), value=pad_token_id)\n",
        "        else:\n",
        "            padded_question_tensor = numericalized_question[:max_len_input]\n",
        "\n",
        "        input_tensor = padded_question_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        # 3️⃣ Initialize output sequence with <bos> token\n",
        "        output_sequence = [vocab[\"<bos>\"]]\n",
        "        output_tensor = torch.tensor(output_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # 4️⃣ Generate tokens step by step\n",
        "        for _ in range(max_len_output):\n",
        "            src_padding_mask = MathTransformer.generate_padding_mask(input_tensor, pad_token_id).to(device)\n",
        "            tgt_mask = MathTransformer.generate_square_subsequent_mask(output_tensor.size(1)).to(device)\n",
        "            output = model(input_tensor, output_tensor, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "            next_token_logits = output[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "            output_sequence.append(next_token.item())\n",
        "            if next_token.item() == vocab[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "            # Update output tensor for next step\n",
        "            # Need to pad the output tensor as well if its length exceeds max_len_output\n",
        "            output_tensor = torch.tensor(output_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        # 5️⃣ Decode output sequence to string, ignoring special tokens\n",
        "        predicted_solution_tokens = [\n",
        "            reverse_vocab[token_id]\n",
        "            for token_id in output_sequence\n",
        "            if reverse_vocab[token_id] not in [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "        ]\n",
        "        predicted_solution = \" \".join(predicted_solution_tokens)\n",
        "\n",
        "        return predicted_solution\n",
        "\n",
        "# Example usage:\n",
        "# Evaluate the smaller model\n",
        "print(\"Evaluating model from cell BPExx2pgGrd-...\")\n",
        "# Assuming model is the trained model from cell BPExx2pgGrd-\n",
        "# Assuming test_loader, criterion, device, vocab, pad_token_id are defined\n",
        "\n",
        "# Get max_len_input and max_len_output from your tensors\n",
        "max_len_input = X_train_tensor.shape[1] # Or X_test_tensor.shape[1]\n",
        "max_len_output = y_train_tensor.shape[1] # Or y_test_tensor.shape[1]\n",
        "\n",
        "avg_test_loss, test_accuracy, bleu = evaluate(model, test_loader, criterion, device, vocab, vocab[\"<pad>\"])\n",
        "\n",
        "# Make a prediction on a test example\n",
        "question_to_solve = X_test_cleaned[24] # Get the first question from the cleaned test data\n",
        "\n",
        "predicted_solution = predict_solution(\n",
        "    model,\n",
        "    question_to_solve,\n",
        "    vocab,\n",
        "    reverse_vocab,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    max_len_input,\n",
        "    max_len_output,\n",
        "    vocab[\"<pad>\"]\n",
        ")\n",
        "\n",
        "print(f\"\\nQuestion: {question_to_solve}\")\n",
        "print(f\"Predicted Solution: {predicted_solution}\")\n",
        "print(f\"Actual Solution: {y_test_cleaned.iloc[24]}\") # Assuming y_test_cleaned is available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1a976c3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7497a32-562e-486e-ef48-85e785ce4af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: what is the calculation of  1+7\n",
            "Predicted Solution: step 1 add the ones → 1 + 7 = 8 . so , the answer is 8 . answer 8\n"
          ]
        }
      ],
      "source": [
        "def predict_solution(model, question, vocab, tokenizer, device, max_len_input, max_len_output, pad_token_id):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Preprocess the input question\n",
        "        cleaned_question = clean_text(question)\n",
        "        tokenized_question = tokenizer(cleaned_question)\n",
        "        numericalized_question = numericalize(tokenized_question)\n",
        "\n",
        "        # Pad the input tensor\n",
        "        padded_question = pad_sequences([numericalized_question], max_len_input)\n",
        "        input_tensor = torch.stack(padded_question, dim=0).to(device) # Stack the list of tensors\n",
        "\n",
        "        # Initialize the output sequence with the beginning-of-sequence token\n",
        "        # Assuming <bos> is the start token and its index is 2 based on your previous output\n",
        "        output_sequence = [vocab[\"<bos>\"]]\n",
        "        output_tensor = torch.tensor(output_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Generate the solution token by token\n",
        "        for _ in range(max_len_output):\n",
        "            # Generate masks\n",
        "            src_padding_mask = MathTransformer.generate_padding_mask(input_tensor, pad_token_id).to(device) # Pass pad_token_id\n",
        "            tgt_mask = MathTransformer.generate_square_subsequent_mask(output_tensor.size(1)).to(device)\n",
        "\n",
        "            # Get the model's output\n",
        "            output = model(input_tensor, output_tensor, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "            # Predict the next token (using the last token's output)\n",
        "            next_token_logits = output[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "            # Append the predicted token to the output sequence\n",
        "            output_sequence.append(next_token.item())\n",
        "\n",
        "            # If the predicted token is the end-of-sequence token, stop generating\n",
        "            if next_token.item() == vocab[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "            # Update the output tensor for the next iteration\n",
        "            output_tensor = torch.tensor(output_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        # Decode the output token IDs back to words\n",
        "        # Assuming the vocabulary object has an itos method\n",
        "        itos = vocab.get_itos()\n",
        "        predicted_solution_tokens = [itos[token_id] for token_id in output_sequence]\n",
        "\n",
        "        # Remove special tokens for the final answer\n",
        "        # Define special_tokens if it's not already defined\n",
        "        special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "        predicted_solution = \" \".join([token for token in predicted_solution_tokens if token not in special_tokens])\n",
        "\n",
        "\n",
        "        return predicted_solution\n",
        "\n",
        "# Example usage:\n",
        "question_to_solve = \"what is the calculation of  1+7\"  # Replace with the question you want to ask\n",
        "\n",
        "# Assuming max_len_input and max_len_output are available from your padding step\n",
        "# If not, you might need to define them based on your data's characteristics\n",
        "max_len_input = X_train_tensor.shape[1] # Or a value that makes sense for new input\n",
        "max_len_output = 1200 # Or a value that makes sense for the expected solution length\n",
        "\n",
        "\n",
        "predicted_solution = predict_solution(\n",
        "    model,\n",
        "    question_to_solve,\n",
        "    vocab,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    max_len_input,\n",
        "    max_len_output,\n",
        "    vocab[\"<pad>\"]\n",
        ")\n",
        "\n",
        "print(f\"Question: {question_to_solve}\")\n",
        "print(f\"Predicted Solution: {predicted_solution}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-TgLyt_yiLB"
      },
      "outputs": [],
      "source": [
        "def predict_solution(model, question, vocab, tokenizer, device, max_len_input, max_len_output, pad_token_id):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Preprocess the input question\n",
        "        cleaned_question = clean_text(question)\n",
        "        tokenized_question = tokenizer(cleaned_question)\n",
        "        numericalized_question = numericalize(tokenized_question)\n",
        "\n",
        "        # Pad the input tensor\n",
        "        padded_question = pad_sequences([numericalized_question], max_len_input)\n",
        "        input_tensor = torch.stack(padded_question, dim=0).to(device) # Stack the list of tensors\n",
        "\n",
        "        # Initialize the output sequence with the beginning-of-sequence token\n",
        "        # Assuming <bos> is the start token and its index is 2 based on your previous output\n",
        "        output_sequence = [vocab[\"<bos>\"]]\n",
        "        output_tensor = torch.tensor(output_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # Generate the solution token by token\n",
        "        for _ in range(max_len_output):\n",
        "            # Generate masks\n",
        "            src_padding_mask = SmallerMathTransformer.generate_padding_mask(input_tensor, pad_token_id).to(device) # Pass pad_token_id\n",
        "            tgt_mask = SmallerMathTransformer.generate_square_subsequent_mask(output_tensor.size(1)).to(device)\n",
        "\n",
        "            # Get the model's output\n",
        "            output = model(input_tensor, output_tensor, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "            # Predict the next token (using the last token's output)\n",
        "            next_token_logits = output[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "            # Append the predicted token to the output sequence\n",
        "            output_sequence.append(next_token.item())\n",
        "\n",
        "            # If the predicted token is the end-of-sequence token, stop generating\n",
        "            if next_token.item() == vocab[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "            # Update the output tensor for the next iteration\n",
        "            output_tensor = torch.tensor(output_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        # Decode the output token IDs back to words\n",
        "        # Assuming the vocabulary object has an itos method\n",
        "        itos = vocab.get_itos()\n",
        "        predicted_solution_tokens = [itos[token_id] for token_id in output_sequence]\n",
        "\n",
        "        # Remove special tokens for the final answer\n",
        "        # Define special_tokens if it's not already defined\n",
        "        special_tokens = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "        predicted_solution = \" \".join([token for token in predicted_solution_tokens if token not in special_tokens])\n",
        "\n",
        "\n",
        "        return predicted_solution\n",
        "\n",
        "# Example usage:\n",
        "question_to_solve = \"5+3\"  # Replace with the question you want to ask\n",
        "\n",
        "# Assuming max_len_input and max_len_output are available from your padding step\n",
        "# If not, you might need to define them based on your data's characteristics\n",
        "max_len_input = X_train_tensor.shape[1] # Or a value that makes sense for new input\n",
        "max_len_output = 1200 # Or a value that makes sense for the expected solution length\n",
        "\n",
        "\n",
        "predicted_solution = predict_solution(\n",
        "    smaller_model,\n",
        "    question_to_solve,\n",
        "    vocab,\n",
        "    tokenizer,\n",
        "    device,\n",
        "    max_len_input,\n",
        "    max_len_output,\n",
        "    vocab[\"<pad>\"]\n",
        ")\n",
        "\n",
        "print(f\"Question: {question_to_solve}\")\n",
        "print(f\"Predicted Solution: {predicted_solution}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnKJ2RPumnhL"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7cdef2b"
      },
      "outputs": [],
      "source": [
        "pip install scikit-learn==1.2.2 numpy==1.24.3 scipy==1.10.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eaf5458"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==1.24.3 scipy==1.10.1 scikit-learn==1.2.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7b4f31e1"
      },
      "outputs": [],
      "source": [
        "# Define the desired number of samples\n",
        "train_samples = 10000\n",
        "test_samples = 1000\n",
        "\n",
        "# Sample the datasets\n",
        "# Use .sample() to randomly select rows\n",
        "# use .reset_index(drop=True) to reset the index after sampling\n",
        "X_train_sampled = X_train.sample(n=min(train_samples, len(X_train)), random_state=42).reset_index(drop=True)\n",
        "y_train_sampled = y_train.sample(n=min(train_samples, len(y_train)), random_state=42).reset_index(drop=True) # Use the same random_state for consistency if needed, or sample independently\n",
        "\n",
        "X_test_sampled = X_test.sample(n=min(test_samples, len(X_test)), random_state=42).reset_index(drop=True)\n",
        "y_test_sampled = y_test.sample(n=min(test_samples, len(y_test)), random_state=42).reset_index(drop=True) # Use the same random_state for consistency if needed, or sample independently\n",
        "\n",
        "\n",
        "print(\"Sampled X_train shape:\", X_train_sampled.shape)\n",
        "print(\"Sampled y_train shape:\", y_train_sampled.shape)\n",
        "print(\"Sampled X_test shape:\", X_test_sampled.shape)\n",
        "print(\"Sampled y_test shape:\", y_test_sampled.shape)\n",
        "\n",
        "# Now, use these sampled variables in your data cleaning cell\n",
        "X_train = X_train_sampled\n",
        "y_train = y_train_sampled\n",
        "X_test = X_test_sampled\n",
        "y_test = y_test_sampled"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bc80d17"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Device setup (re-using the previous device)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"✅ Using device:\", device)\n",
        "\n",
        "# Positional Encoding (re-using the previous class)\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, embed_size, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, embed_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * position * div_term) # Corrected\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]\n",
        "\n",
        "# Smaller Transformer Model with adjustable parameters\n",
        "class SmallerMathTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, num_encoder_layers, num_decoder_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(embed_size)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=embed_size,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_padding_mask=None, tgt_mask=None):\n",
        "        src = self.embedding(src)\n",
        "        src = self.positional_encoding(src)\n",
        "\n",
        "        tgt = self.embedding(tgt)\n",
        "        tgt = self.positional_encoding(tgt)\n",
        "\n",
        "        output = self.transformer(\n",
        "            src, tgt,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "        return self.fc_out(output)\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_square_subsequent_mask(sz):\n",
        "        mask = torch.triu(torch.ones(sz, sz), diagonal=1).bool()\n",
        "        return mask.masked_fill(mask, float('-inf')).masked_fill(~mask, float(0.0))\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_padding_mask(seq, pad_token_id):\n",
        "        return seq == pad_token_id\n",
        "\n",
        "    @staticmethod\n",
        "    def handle_vocab_size(X_batch, y_batch, vocab_size, unk_token=1):\n",
        "        X_batch = X_batch.clone()\n",
        "        y_batch = y_batch.clone()\n",
        "        X_batch[X_batch >= vocab_size] = unk_token\n",
        "        y_batch[y_batch >= vocab_size] = unk_token\n",
        "        return X_batch, y_batch\n",
        "\n",
        "# Hyperparameters for the smaller model\n",
        "# You can adjust these values to experiment with different sizes\n",
        "smaller_embed_size = 128  # Reduced embedding size\n",
        "smaller_num_heads = 4      # Reduced number of attention heads\n",
        "smaller_num_layers = 2     # Reduced number of encoder/decoder layers\n",
        "smaller_dropout = 0.1      # Dropout rate\n",
        "\n",
        "# Initialize the smaller model\n",
        "smaller_model = SmallerMathTransformer(\n",
        "    vocab_size=len(vocab),  # Using the vocabulary size from the previous step\n",
        "    embed_size=smaller_embed_size,\n",
        "    num_heads=smaller_num_heads,\n",
        "    num_encoder_layers=smaller_num_layers,\n",
        "    num_decoder_layers=smaller_num_layers,\n",
        "    dropout=smaller_dropout\n",
        ").to(device)\n",
        "\n",
        "# Loss function and optimizer (can be the same as before)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
        "optimizer = optim.Adam(smaller_model.parameters(), lr=1e-5, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.8) # Or adjust as needed\n",
        "\n",
        "# Training loop (similar to the previous one, but using smaller_model)\n",
        "smaller_loss_values = []\n",
        "num_epochs = 5 # You might want to train for more epochs with a smaller model initially\n",
        "\n",
        "print(\"\\nStarting training for the smaller model...\")\n",
        "for epoch in range(num_epochs):\n",
        "    smaller_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # Handle vocab overflow\n",
        "        X_batch, y_batch = SmallerMathTransformer.handle_vocab_size(X_batch, y_batch, len(vocab), vocab[\"<unk>\"])\n",
        "\n",
        "        # Padding and masks\n",
        "        src_padding_mask = SmallerMathTransformer.generate_padding_mask(X_batch, vocab[\"<pad>\"]).to(device)\n",
        "        tgt_input = y_batch[:, :-1]\n",
        "        tgt_output = y_batch[:, 1:]\n",
        "\n",
        "        tgt_mask = SmallerMathTransformer.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        optimizer.zero_grad()\n",
        "        output = smaller_model(X_batch, tgt_input, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "        # Reshape for loss\n",
        "        output = output.reshape(-1, len(vocab))\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "        loss = criterion(output, tgt_output)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(smaller_model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    smaller_loss_values.append(avg_loss)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"✅ Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Plot loss for the smaller model\n",
        "plt.plot(smaller_loss_values, marker='o', label=\"Smaller Model Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Smaller Model Training Loss Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nTraining of the smaller model completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3807470"
      },
      "source": [
        "# Evaluate the smaller model\n",
        "smaller_model.eval()  # Set the model to evaluation mode\n",
        "total_test_loss = 0\n",
        "\n",
        "with torch.no_grad():  # Disable gradient computation during evaluation\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "        # Handle vocab overflow\n",
        "        X_batch, y_batch = SmallerMathTransformer.handle_vocab_size(X_batch, y_batch, len(vocab), vocab[\"<unk>\"])\n",
        "\n",
        "        # Padding mask\n",
        "        src_padding_mask = SmallerMathTransformer.generate_padding_mask(X_batch, vocab[\"<pad>\"]).to(device)\n",
        "        tgt_input = y_batch[:, :-1]  # All tokens except the last one\n",
        "        tgt_output = y_batch[:, 1:]  # All tokens except the first one\n",
        "\n",
        "        # Generate the target mask for the transformer\n",
        "        tgt_mask = SmallerMathTransformer.generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        output = smaller_model(X_batch, tgt_input, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        output = output.reshape(-1, len(vocab))\n",
        "        tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output, tgt_output)\n",
        "        total_test_loss += loss.item()\n",
        "\n",
        "avg_test_loss = total_test_loss / len(test_loader)\n",
        "\n",
        "print(f\"✅ Smaller Model Evaluation Loss: {avg_test_loss:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️⃣ Pad input sequence\n",
        "padded_question = pad_sequences([numericalized_question], max_len_input)\n",
        "\n",
        "# Ensure it’s numeric\n",
        "try:\n",
        "    import numpy as np\n",
        "    padded_question = np.array(padded_question, dtype=int)\n",
        "except:\n",
        "    # fallback for nested list\n",
        "    padded_question = [list(map(int, seq)) for seq in padded_question]\n",
        "\n",
        "input_tensor = torch.tensor(padded_question, dtype=torch.long).to(device)\n",
        "if input_tensor.dim() == 1:\n",
        "    input_tensor = input_tensor.unsqueeze(0)\n"
      ],
      "metadata": {
        "id": "kTFGNWtf8Gcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "reverse_vocab = {idx: token for token, idx in vocab.get_stoi().items()}  # for torchtext Vocab\n",
        "\n",
        "def predict_solution(model, question, vocab, reverse_vocab, tokenizer, device, max_len_input, max_len_output, pad_token_id):\n",
        "    \"\"\"\n",
        "    Predicts the solution to a math question using a trained Transformer model.\n",
        "\n",
        "    Args:\n",
        "        model: Trained Transformer model\n",
        "        question: String, math question to solve\n",
        "        vocab: Vocab object (token -> index)\n",
        "        reverse_vocab: dict (index -> token) for decoding\n",
        "        tokenizer: Function to tokenize the question\n",
        "        device: torch.device\n",
        "        max_len_input: Max length of input sequence\n",
        "        max_len_output: Max length of output sequence\n",
        "        pad_token_id: ID of the padding token\n",
        "\n",
        "    Returns:\n",
        "        predicted_solution: String, the model's predicted solution\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # 1️⃣ Preprocess the question\n",
        "        cleaned_question = clean_text(question)  # your text cleaning function\n",
        "        tokenized_question = tokenizer(cleaned_question)\n",
        "        numericalized_question = numericalize(tokenized_question)  # convert tokens -> indices\n",
        "\n",
        "        # 2️⃣ Pad input sequence\n",
        "        padded_question = pad_sequences([numericalized_question], max_len_input)\n",
        "        input_tensor = torch.tensor(padded_question, dtype=torch.long).to(device)\n",
        "        if input_tensor.dim() == 1:\n",
        "            input_tensor = input_tensor.unsqueeze(0)\n",
        "\n",
        "        # 3️⃣ Initialize output sequence with <bos> token\n",
        "        output_sequence = [vocab[\"<bos>\"]]\n",
        "        output_tensor = torch.tensor(output_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # 4️⃣ Generate tokens step by step\n",
        "        for _ in range(max_len_output):\n",
        "            src_padding_mask = SmallerMathTransformer.generate_padding_mask(input_tensor, pad_token_id).to(device)\n",
        "            tgt_mask = SmallerMathTransformer.generate_square_subsequent_mask(output_tensor.size(1)).to(device)\n",
        "            output = model(input_tensor, output_tensor, src_padding_mask=src_padding_mask, tgt_mask=tgt_mask)\n",
        "\n",
        "            next_token_logits = output[:, -1, :]\n",
        "            next_token = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "            output_sequence.append(next_token.item())\n",
        "            if next_token.item() == vocab[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "            # Update output tensor for next step\n",
        "            output_tensor = torch.tensor(output_sequence, dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "        # 5️⃣ Decode output sequence to string, ignoring special tokens\n",
        "        predicted_solution_tokens = [\n",
        "            reverse_vocab[token_id]\n",
        "            for token_id in output_sequence\n",
        "            if reverse_vocab[token_id] not in [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "        ]\n",
        "        predicted_solution = \" \".join(predicted_solution_tokens)\n",
        "\n",
        "        return predicted_solution\n",
        "\n",
        "# --------------------------\n",
        "# Example usage\n",
        "# --------------------------\n",
        "question_to_solve = \"5 + 3\"\n",
        "max_len_input = X_train_tensor.shape[1]\n",
        "max_len_output = 100  # you can adjust based on expected solution length\n",
        "\n",
        "predicted_solution = predict_solution(\n",
        "    smaller_model,\n",
        "    question_to_solve,\n",
        "    vocab,\n",
        "    reverse_vocab,  # ✅ reverse_vocab must be defined as {index: token}\n",
        "    tokenizer,\n",
        "    device,\n",
        "    max_len_input,\n",
        "    max_len_output,\n",
        "    vocab[\"<pad>\"]\n",
        ")\n",
        "\n",
        "print(f\"Question: {question_to_solve}\")\n",
        "print(f\"Predicted Solution: {predicted_solution}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "xrU9SEDH3s29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NxxUSfZE6nhu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1Rw6JbTfBlVKLro1lnitrLvHgogztjZVz",
      "authorship_tag": "ABX9TyMkLCxiqQA+3RyEHSEaxqZt",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}