{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJPpJHpmJsUIOwm8/uSU5V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Narmathan56/AI-Math-Tutor/blob/main/Data_Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znFt4s4TBAvj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3euiAaNosXRx",
        "outputId": "25b1cbb6-99ba-45de-fc74-864225efbcb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Sample Cleaned Output: 0                what is 8 + 8?\n",
            "1                      7 plus 0\n",
            "2              calculate 3 + 8.\n",
            "3                      5 plus 1\n",
            "4                         0 + 5\n",
            "                 ...           \n",
            "100        can you add 8 and 0?\n",
            "101                add 5 and 3.\n",
            "102                    9 plus 9\n",
            "103                       9 + 1\n",
            "104    find the sum of 4 and 5.\n",
            "Name: question, Length: 100, dtype: object\n",
            "âœ… Sample Cleaned Output: step 1: add the ones â†’ 8 + 8 = 16. so, the answer is 16. answer: 16\n",
            "X_train_cleaned_type <class 'str'>\n",
            "âœ… Preprocessing Done!\n",
            "X_train_tensor shape: torch.Size([647, 10])\n",
            "y_train_tensor shape: torch.Size([647, 30])\n",
            "X_test_tensor shape: torch.Size([647, 10])\n",
            "y_test_tensor shape: torch.Size([647, 30])\n",
            "ðŸ“š Vocabulary size: 51\n",
            "tensor([31, 10, 17,  8, 17, 25,  1,  1,  1,  1])\n",
            "tensor([ 2, 16,  7,  9,  5, 13, 15, 17,  8, 17, 12, 48,  4, 14, 11,  5,  6, 10,\n",
            "        48,  4,  6, 48,  3,  1,  1,  1,  1,  1,  1,  1])\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.nn.functional import pad\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import vocab as torch_vocab\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Step 1: Clean text function\n",
        "# Decode entire column first\n",
        "X_train = X_train.apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
        "y_train = y_train.apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
        "X_test = X_test.apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
        "y_test = y_test.apply(lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x)\n",
        "\n",
        "def clean_text(text):\n",
        "    if text is None or pd.isna(text):\n",
        "       return \"\"\n",
        "   # if(text.startswith(\"b'\")and text.endswith(\"'\")):\n",
        "\n",
        "       # text=text[2:-3].strip()\n",
        "    # Decode bytes literal if necessary\n",
        "    if isinstance(text, bytes):\n",
        "        text = text.decode(\"utf-8\")\n",
        "    # Assuming UTF-8 encoding\n",
        "\n",
        "    # Extract labels from Asymptote block before removing it\n",
        "    asy_labels = re.findall(r'label\\([^,]+,\"([^\"]+)\"', text)\n",
        "\n",
        "    # Remove entire [asy] block\n",
        "    text = re.sub(r'\\[asy\\].*?\\[/asy\\]','', text, flags=re.DOTALL)\n",
        "\n",
        "\n",
        "\n",
        "    # Remove LaTeX math mode ($...$), LaTeX commands like \\mbox{}, etc.\n",
        "    text = re.sub(r'\\$\\\\?([^$]+)\\\\?\\$', r'\\1', text)\n",
        "    text = re.sub(r'\\\\\\w+\\{(.*?)\\}', r'\\1', text)\n",
        "    text = text.replace('\\\\', '')\n",
        "\n",
        "    # Normalize whitespaces and math operators\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = re.sub(r'\\s*([+\\-*/=()^])\\s*', r' \\1 ', text)\n",
        "\n",
        "    # Fix parentheses spacing\n",
        "    text = re.sub(r'\\s*\\(\\s*', '(', text)\n",
        "    text = re.sub(r'\\s*\\)\\s*', ')', text)\n",
        "\n",
        "    # Lowercase everything\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove trailing newline characters explicitly if not removed by strip()\n",
        "    text = text.rstrip('\\n')\n",
        "\n",
        "\n",
        "    # Add logic sentence from Asymptote if present\n",
        "    if asy_labels:\n",
        "        logic_steps = [label.lower() for label in asy_labels if \"input\" not in label and \"output\" not in label]\n",
        "        logic_sentence = \" the machine \" + \", then \".join(logic_steps) + \".\" if logic_steps else \"\"\n",
        "\n",
        "        input_value = next((label.split('=')[1].strip() for label in asy_labels if \"input\" in label), None)\n",
        "\n",
        "        if input_value:\n",
        "            text = f\"in the function machine, the input is {input_value}. {text}{logic_sentence} what is the output?\"\n",
        "        else:\n",
        "            text = f\"{text}{logic_sentence} what is the output?\"\n",
        "\n",
        "    return text\n",
        "\n",
        "# Step 2: Clean datasets\n",
        "# we are insert the system which've already made by us\n",
        "X_train_cleaned = X_train.apply(clean_text)\n",
        "y_train_cleaned = y_train.astype(str).apply(clean_text)\n",
        "X_test_cleaned = X_test.apply(clean_text)\n",
        "y_test_cleaned = y_test.astype(str).apply(clean_text)\n",
        "\n",
        "print(\"âœ… Sample Cleaned Output:\", X_train_cleaned.iloc[0 :100])\n",
        "print(\"âœ… Sample Cleaned Output:\", y_train_cleaned.iloc[0])\n",
        "print(\"X_train_cleaned_type\",type(X_train.iloc[0]))\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "X_train_tokens = X_train_cleaned.apply(tokenizer)\n",
        "y_train_tokens = y_train_cleaned.apply(tokenizer)\n",
        "X_test_tokens = X_test_cleaned.apply(tokenizer)\n",
        "y_test_tokens = y_test_cleaned.apply(tokenizer)\n",
        "\n",
        "# Step 4: Build vocab from all tokens\n",
        "def yield_tokens(*data_parts):\n",
        "    for part in data_parts:\n",
        "        for tokens in part:\n",
        "            yield tokens\n",
        "\n",
        "vocab = build_vocab_from_iterator(\n",
        "    yield_tokens(X_train_tokens, y_train_tokens, X_test_tokens, y_test_tokens),\n",
        "    specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"],\n",
        "    max_tokens=5000\n",
        ")\n",
        "\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Step 5: Numericalize\n",
        "def numericalize(token_list, add_bos_eos=False):\n",
        "    if add_bos_eos:\n",
        "        token_list = ['<bos>'] + token_list + ['<eos>']\n",
        "    return torch.tensor([vocab[token] for token in token_list])\n",
        "\n",
        "X_train_ids = X_train_tokens.apply(lambda x: numericalize(x))\n",
        "y_train_ids = y_train_tokens.apply(lambda x: numericalize(x, add_bos_eos=True))\n",
        "X_test_ids = X_test_tokens.apply(lambda x: numericalize(x))\n",
        "y_test_ids = y_test_tokens.apply(lambda x: numericalize(x, add_bos_eos=True))\n",
        "\n",
        "# Step 6: Padding\n",
        "def pad_sequences(sequences, max_len):\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        # Ensure seq is a tensor before padding\n",
        "        if not isinstance(seq, torch.Tensor):\n",
        "            seq = torch.tensor(seq)\n",
        "\n",
        "        current_len = seq.size(0)\n",
        "        if current_len < max_len:\n",
        "            padded_seq = pad(seq, (0, max_len - current_len), value=vocab[\"<pad>\"])\n",
        "        else:\n",
        "            padded_seq = seq[:max_len]\n",
        "        padded_sequences.append(padded_seq)\n",
        "\n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "X_train_tensor = pad_sequences(X_train_ids.tolist(), max_len=10)\n",
        "y_train_tensor = pad_sequences(y_train_ids.tolist(), max_len=30)\n",
        "X_test_tensor = pad_sequences(X_test_ids.tolist(), max_len=10)\n",
        "y_test_tensor = pad_sequences(y_test_ids.tolist(), max_len=30)\n",
        "\n",
        "\n",
        "# Step 7: Stack into final tensors\n",
        "X_train_tensor = torch.stack(X_train_tensor)\n",
        "y_train_tensor = torch.stack(y_train_tensor)\n",
        "X_test_tensor = torch.stack(X_test_tensor)\n",
        "y_test_tensor = torch.stack(y_test_tensor)\n",
        "\n",
        "\n",
        "# Step 8: Output shapes\n",
        "print(\"âœ… Preprocessing Done!\")\n",
        "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
        "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
        "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
        "print(\"y_test_tensor shape:\", y_test_tensor.shape)\n",
        "print(\"ðŸ“š Vocabulary size:\", len(vocab))\n",
        "print(X_train_tensor[0])\n",
        "print(y_train_tensor[0])"
      ]
    }
  ]
}